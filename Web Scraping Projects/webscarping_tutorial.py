# -*- coding: utf-8 -*-
"""Webscarping Tutorial.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12nGxe-IUu-_pJPZzKaQs5dEGi5ljIv3d

BeatifulSoup and Requests
"""

## The are the two main packages used for webscraping,
## WE will firstly import these two parkages

from bs4 import BeautifulSoup
import requests

url = 'https://www.scrapethissite.com/pages/forms/?page_num=1&per_page=100'

## we will use request.get to access the url
page = requests.get(url)

## we will import te entire html page of the website fr the purpose of inspecting it and getting the table needed
soup = BeautifulSoup(page.text, 'html')

print(soup)

## prettify is just to make the html file looks really nice to view
print(soup.prettify)

"""Using the find and find_all to look through the html file of the page we want to scrap"""

## using the find: The return the first reponse in our html list
## fid_all returns all it can find on a request

soup.find('div')

soup.find_all('div')

## notice the difference and to filter add a comma then specify the attribute that i am looking for

soup.find_all('div', class_ ='col-md-12')
# we add an underscore to class becuse class is also an attribute in python

soup.find_all('p')

soup.find_all('p', class_ = 'lead')

soup.find('p', class_ = 'lead').text

"""Note: It is impossible to use find all or text extraaction but it best for surfng through the html file faster"""

soup.find('p', class_ = 'lead').text.strip()
#.strip is to remove the wild space in from the text extracted

soup.find('h1').text.strip()

# trying to pull nly the column of the team ame
soup.find('th').text.strip()

# Find all 'th' elements
column_names = [th.get_text(strip=True) for th in soup.find_all('th')]

# Display the list of column names
print(column_names)

# Find all 'th' elements and get the text of the first three
column_names = [th.get_text(strip=True) for th in soup.find_all('th')]

# Display the list of the first three column names
print(column_names)

#importing the panda library so as to help save the data in a dataframe to be
# for analysis
import pandas as pd

df = pd.DataFrame(columns=column_names)
df

table_data = soup.find_all('tr')

for row in table_data[1:]:
  row_data = row.find_all('td')
  main_table_data = [main_data.text.strip() for main_data in row_data]
  length = len(df)
  df.loc[length] = main_table_data

df

df.to_csv("practise hockey webscraping")





############################################################################################
############################################################################################
############################################################################################
############################################################################################
# It should be noted that the page has multiple pages with the continuation f the tabe being extracted.
# The above demonstration only explain how to scrap a single web page while to srcap all in the webpage
# You will have to loop through the first page to the number of pages that tables streched to.
# The below code explain how to scrap all the table in the pages for teh url given
import requests
from bs4 import BeautifulSoup
import pandas as pd

# Base URL with placeholders for the page number
base_url = 'https://www.scrapethissite.com/pages/forms/?page_num={}&per_page=100'

# Initialize an empty list to store all rows
all_rows = []

# Loop through pages 1 to 100
for page_num in range(1, 100):
    # Fetch the page content
    url = base_url.format(page_num)
    page = requests.get(url)
    soup = BeautifulSoup(page.text, 'html')

    # Extract table headers only once
    if page_num == 1:
        column_names = [th.text.strip() for th in soup.find_all('th')]

    # Extract table rows
    table_rows = soup.find_all('tr')
    for row in table_rows[1:]:  # Skip header row
        row_data = row.find_all('td')
        main_table_data = [data.text.strip() for data in row_data]
        all_rows.append(main_table_data)

# Convert the list of rows into a DataFrame
df = pd.DataFrame(all_rows, columns=column_names)

# Save the DataFrame to a CSV file
df.to_csv('hockey_data_pages_1_to_100.csv', index=False)

print("Scraped data saved to hockey_data_pages_1_to_100.csv")

